{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import Flatten\n",
    "import torchvision \n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aarso2/BPI/Projeto_Semana6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetEmotions(torch.utils.data.Dataset):\n",
    "    def __init__(self,path, transforms = None):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.images = glob.glob(os.path.join(self.path,\"*_oneface/*\"))\n",
    "        self.labels = {\"anger\":0,\"joy\":1,\"neutral\":2}\n",
    "        self.transforms = transforms\n",
    "    def __getitem__(self,idx):\n",
    "        image = cv2.imread(self.images[idx])\n",
    "        if self.transforms != None: \n",
    "            image = self.transforms(image)\n",
    "        return image, self.labels[self.images[idx].split(\"/\")[-2].replace(\"_oneface\", \"\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your data folder and optional image transformations\n",
    "train_path = cwd+'/BPI-MELD-main/train/train_splits_images/'\n",
    "dev_path = cwd+'/BPI-MELD-main/dev/dev_splits_images/'\n",
    "\n",
    "transforming = T.Compose([T.Resize((112, 112)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Create the custom dataset and data loader\n",
    "train_dataset = DatasetEmotions(train_path, transforms=transforming)\n",
    "dev_dataset = DatasetEmotions(dev_path, transforms=transforming)\n",
    "train_data = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_data = DataLoader(dev_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras no conjunto de dados de treino: 5760\n",
      "Total de amostras no conjunto de dados de teste: 672\n"
     ]
    }
   ],
   "source": [
    "total_samples_train = len(train_data) * train_data.batch_size\n",
    "total_samples_dev = len(dev_data) * dev_data.batch_size\n",
    "print(f\"Total de amostras no conjunto de dados de treino: {total_samples_train}\")\n",
    "print(f\"Total de amostras no conjunto de dados de teste: {total_samples_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class EmotionRecognitionNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionRecognitionNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * (112 // 8) * (112 // 8), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of emotion classes\n",
    "num_classes = 3\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = EmotionRecognitionNet(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (assuming you have train_data DataLoader)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_data):\n",
    "        # Move inputs and targets to the device (CPU or GPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch_idx + 1}/{len(train_data)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Validation (assuming you have dev_data DataLoader)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets in dev_data:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
